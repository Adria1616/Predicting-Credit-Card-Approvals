{"cells":[{"source":"![Credit card being held in hand](credit_card.jpg)\n\nCommercial banks receive _a lot_ of applications for credit cards. Many of them get rejected for many reasons, like high loan balances, low income levels, or too many inquiries on an individual's credit report, for example. Manually analyzing these applications is mundane, error-prone, and time-consuming (and time is money!). Luckily, this task can be automated with the power of machine learning and pretty much every commercial bank does so nowadays. In this workbook, you will build an automatic credit card approval predictor using machine learning techniques, just like real banks do.\n\n### The Data\n\nThe data is a small subset of the Credit Card Approval dataset from the UCI Machine Learning Repository showing the credit card applications a bank receives. This dataset has been loaded as a `pandas` DataFrame called `cc_apps`. The last column in the dataset is the target value.","metadata":{},"id":"35aebf2e-0635-4fef-bc9a-877b6a20fb13","cell_type":"markdown"},{"source":"\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\n\n\ncc_apps = pd.read_csv(\"cc_approvals.data\", header=None) \nprint(cc_apps.head(100))\n\n#1) Cleaning data\ncc_apps_corrected=cc_apps.replace(\"?\",np.NAN)\n#2) Filling missing values \ndef fill_missing_values(df):\n    for column in df.columns:\n        if df[column].dtypes== 'object':\n           df[column]= df[column].fillna(\n             df[column].value_counts().index[0] \n           )\n        else:\n           df[column]= df[column].fillna(df[column].mean())\n    return df\n    \ncc_apps_corrected=fill_missing_values(cc_apps_corrected)\n#3) changing values into numbers\ncc_apps_clean=pd.get_dummies(cc_apps_corrected,drop_first=True)\n\nprint(cc_apps_clean.head(10))\n#4) Defining dependent and independent variables\nX=cc_apps_clean.iloc[:, :-1].values #all rows and coumns except the last one\ny=cc_apps_clean.iloc[:, [-1]].values #all rows only in the last column\n\n#5) Dividing intro trainning and test \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n\n#6) Standarizing numeric data\nscaler=StandardScaler()\nrescaledX_train = scaler.fit_transform(X_train)\nrescaledX_test = scaler.transform(X_test)\n\n#7) Creating the model\nlogreg = LogisticRegression()\n\n#8)Traning the model\nlogreg.fit(rescaledX_train, y_train)\n\n#9) Make the predictions\ny_train_pred = logreg.predict(rescaledX_train)\n\n#10) Print the confusion matrix of the logreg model\nprint(confusion_matrix(y_train, y_train_pred))\n\n#11)Establishing the grid values \ntol = [0.01, 0.001, 0.0001]\nmax_iter = [120, 170, 200]\n\n#12)reating a dictionary with the results\nmodel_dictionary={\"tol\":tol,\"max_iter\":max_iter}\n\n#13) It sets up the grid search process\ngrid_model = GridSearchCV(estimator=logreg, param_grid=model_dictionary, cv=5)\n\n#14) Starts the process of training with the different combinations\ngrid_model_result = grid_model.fit(rescaledX_train, y_train)\n\n#15) Results, saving into two variable names\nbest_train_score, best_train_params = grid_model_result.best_score_, grid_model_result.best_params_\n\n#16)Pringting the best results\nprint(\"Best: %f using %s\" % (best_train_score, best_train_params))\n\n#17) Extracting the best model\nbest_model = grid_model_result.best_estimator_\nbest_score =  best_model.score(rescaledX_test, y_test)\n\n#18 Printing the best score\nprint(\"Accuracy of logistic regression classifier: \", best_score)\n\nprint(\"Your model expects:\", X_train.shape[1], \"values per row.\")\n","metadata":{"executionCancelledAt":null,"executionTime":7054,"lastExecutedAt":1746574352242,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\n\n# Load the dataset\ncc_apps = pd.read_csv(\"cc_approvals.data\", header=None) \nprint(cc_apps.head(100))\n\n#1) Cleaning data\ncc_apps_corrected=cc_apps.replace(\"?\",np.NAN)\n#2) Filling missing values \ndef fill_missing_values(df):\n    for column in df.columns:\n        if df[column].dtypes== 'object':\n           df[column]= df[column].fillna(\n             df[column].value_counts().index[0] \n           )\n        else:\n           df[column]= df[column].fillna(df[column].mean())\n    return df\n    \ncc_apps_corrected=fill_missing_values(cc_apps_corrected)\n#3) changing values into numbers\ncc_apps_clean=pd.get_dummies(cc_apps_corrected,drop_first=True)\n\nprint(cc_apps_clean.head(10))\n#4) Defining dependent and independent variables\nX=cc_apps_clean.iloc[:, :-1].values #all rows and coumns except the last one\ny=cc_apps_clean.iloc[:, [-1]].values #all rows only in the last column\n\n#5) Dividing intro trainning and test \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n\n#6) Standarizing numeric data\nscaler=StandardScaler()\nrescaledX_train = scaler.fit_transform(X_train)\nrescaledX_test = scaler.transform(X_test)\n\n#7) Creating the model\nlogreg = LogisticRegression()\n\n#8)Traning the model\nlogreg.fit(rescaledX_train, y_train)\n\n#9) Make the predictions\ny_train_pred = logreg.predict(rescaledX_train)\n\n#10) Print the confusion matrix of the logreg model\nprint(confusion_matrix(y_train, y_train_pred))\n\n#11)Establishing the grid values \ntol = [0.01, 0.001, 0.0001]\nmax_iter = [120, 170, 200]\n\n#12)reating a dictionary with the results\nmodel_dictionary={\"tol\":tol,\"max_iter\":max_iter}\n\n#13) It sets up the grid search process\ngrid_model = GridSearchCV(estimator=logreg, param_grid=model_dictionary, cv=5)\n\n#14) Starts the process of training with the different combinations\ngrid_model_result = grid_model.fit(rescaledX_train, y_train)\n\n#15) Results, saving into two variable names\nbest_train_score, best_train_params = grid_model_result.best_score_, grid_model_result.best_params_\n\n#16)Pringting the best results\nprint(\"Best: %f using %s\" % (best_train_score, best_train_params))\n\n#17) Extracting the best model\nbest_model = grid_model_result.best_estimator_\nbest_score =  best_model.score(rescaledX_test, y_test)\n\n#18 Printing the best score\nprint(\"Accuracy of logistic regression classifier: \", best_score)\n\nprint(\"Your model expects:\", X_train.shape[1], \"values per row.\")\n","outputsMetadata":{"0":{"height":483,"type":"stream"}},"lastExecutedByKernel":"0af15e57-bea2-485a-be77-4b103d40452e"},"id":"6e86b1e8-a3fa-4b09-982f-795f218bd1a6","cell_type":"code","execution_count":68,"outputs":[{"output_type":"stream","name":"stdout","text":"   0      1       2  3  4   5   6      7  8  9   10 11   12 13\n0   b  30.83   0.000  u  g   w   v  1.250  t  t   1  g    0  +\n1   a  58.67   4.460  u  g   q   h  3.040  t  t   6  g  560  +\n2   a  24.50   0.500  u  g   q   h  1.500  t  f   0  g  824  +\n3   b  27.83   1.540  u  g   w   v  3.750  t  t   5  g    3  +\n4   b  20.17   5.625  u  g   w   v  1.710  t  f   0  s    0  +\n.. ..    ...     ... .. ..  ..  ..    ... .. ..  .. ..  ... ..\n95  a  28.58   3.540  u  g   i  bb  0.500  t  f   0  g    0  -\n96  b  23.00   0.625  y  p  aa   v  0.125  t  f   0  g    1  -\n97  b      ?   0.500  u  g   c  bb  0.835  t  f   0  s    0  -\n98  a  22.50  11.000  y  p   q   v  3.000  t  f   0  g    0  -\n99  a  28.50   1.000  u  g   q   v  1.000  t  t   2  g  500  -\n\n[100 rows x 14 columns]\n        2      7  10     12  0_b  1_15.17  ...  6_z  8_t  9_t  11_p  11_s  13_-\n0   0.000  1.250   1      0    1        0  ...    0    1    1     0     0     0\n1   4.460  3.040   6    560    0        0  ...    0    1    1     0     0     0\n2   0.500  1.500   0    824    0        0  ...    0    1    0     0     0     0\n3   1.540  3.750   5      3    1        0  ...    0    1    1     0     0     0\n4   5.625  1.710   0      0    1        0  ...    0    1    0     0     1     0\n5   4.000  2.500   0      0    1        0  ...    0    1    0     0     0     0\n6   1.040  6.500   0  31285    1        0  ...    0    1    0     0     0     0\n7  11.585  0.040   0   1349    0        0  ...    0    1    0     0     0     0\n8   0.500  3.960   0    314    1        0  ...    0    1    0     0     0     0\n9   4.915  3.165   0   1442    1        0  ...    0    1    0     0     0     0\n\n[10 rows x 383 columns]\n[[234   3]\n [  3 312]]\nBest: 0.842391 using {'max_iter': 120, 'tol': 0.01}\nAccuracy of logistic regression classifier:  0.782608695652174\nYour model expects: 382 values per row.\n"}]},{"source":"# 19) Prediction using original data\n\n# Creating a new data frame with the new values\nnew_input_df = pd.DataFrame([[\n    'a', 10.83, 45.000, 'u', 'g', 'w', 'v', 2.250, 't', 't', 3, 'g', 560\n]])\n\n# Repeting same steps\nnew_input_df.columns = cc_apps.columns[:-1]\n\n# Cleaning data\nnew_input_filled = fill_missing_values(new_input_df)\nnew_input_encoded = pd.get_dummies(new_input_filled, drop_first=True)\n\n# Aligning columns with the trainning\nnew_input_aligned = new_input_encoded.reindex(columns=cc_apps_clean.columns[:-1], fill_value=0)\n\n# Matching with the column names\nnew_input_aligned.columns = new_input_aligned.columns.astype(str)\n\n# Escaling\nnew_input_scaled = scaler.transform(new_input_aligned)\n\n# Predicting data\nprediction = best_model.predict(new_input_scaled)\nprint(\"Prediction:\", \"Not Approved\" if prediction[0] == 1 else \"Approved\")\n","metadata":{"executionCancelledAt":null,"executionTime":99,"lastExecutedAt":1746574352341,"lastExecutedByKernel":"0af15e57-bea2-485a-be77-4b103d40452e","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# 19) Prediction using original data\n\n# Creating a new data frame with the new values\nnew_input_df = pd.DataFrame([[\n    'a', 10.83, 45.000, 'u', 'g', 'w', 'v', 2.250, 't', 't', 3, 'g', 560\n]])\n\n# Repeting same steps\nnew_input_df.columns = cc_apps.columns[:-1]\n\n# Cleaning data\nnew_input_filled = fill_missing_values(new_input_df)\nnew_input_encoded = pd.get_dummies(new_input_filled, drop_first=True)\n\n# Aligning columns with the trainning\nnew_input_aligned = new_input_encoded.reindex(columns=cc_apps_clean.columns[:-1], fill_value=0)\n\n# Matching with the column names\nnew_input_aligned.columns = new_input_aligned.columns.astype(str)\n\n# Escaling\nnew_input_scaled = scaler.transform(new_input_aligned)\n\n# Predicting data\nprediction = best_model.predict(new_input_scaled)\nprint(\"Prediction:\", \"Not Approved\" if prediction[0] == 1 else \"Approved\")\n","outputsMetadata":{"0":{"height":38,"type":"stream"}}},"cell_type":"code","id":"39d6a3f4-aa09-49cf-b9e6-207efe7a43db","outputs":[{"output_type":"stream","name":"stdout","text":"Prediction: Not Approved\n"}],"execution_count":69}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"editor":"DataLab"},"nbformat":4,"nbformat_minor":5}